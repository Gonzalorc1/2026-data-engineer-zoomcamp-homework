# Workflow Orchestration con Apache Airflow

Este directorio contiene la implementaci√≥n de los flujos de trabajo usando **Apache Airflow** en lugar de Kestra. Los DAGs (Directed Acyclic Graphs) de Airflow son equivalentes a los flujos YAML de Kestra.

## üìã Estructura del Proyecto

```
021-workflow-orchestration/
‚îú‚îÄ‚îÄ dags/                    # DAGs de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ 07_gcp_setup.py      # Setup inicial de GCP (bucket y dataset)
‚îÇ   ‚îú‚îÄ‚îÄ 08_gcp_taxi.py       # Procesamiento manual de datos de taxis
‚îÇ   ‚îî‚îÄ‚îÄ 09_gcp_taxi_scheduled.py  # Procesamiento programado de datos de taxis
‚îú‚îÄ‚îÄ logs/                    # Logs de Airflow
‚îú‚îÄ‚îÄ plugins/                 # Plugins personalizados (opcional)
‚îú‚îÄ‚îÄ config/                  # Configuraci√≥n adicional
‚îú‚îÄ‚îÄ gcp_keys/                # Credenciales de GCP (no commitear)
‚îú‚îÄ‚îÄ docker-compose.yml        # Configuraci√≥n de Docker Compose
‚îú‚îÄ‚îÄ Dockerfile              # Imagen personalizada de Airflow
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias de Python
‚îú‚îÄ‚îÄ .env                    # Variables de entorno (crear manualmente)
‚îî‚îÄ‚îÄ README.md               # Este archivo
```

## üöÄ Configuraci√≥n Inicial

### 1. Configurar Variables de Entorno

Crea un archivo `.env` en la ra√≠z del proyecto con las siguientes variables:

```bash
# Airflow
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=/ruta/a/tu/proyecto/021-workflow-orchestration

# GCP Configuration
GCP_PROJECT_ID=tu-proyecto-gcp
GCP_DATASET=tu-dataset-name
GCP_BUCKET_NAME=tu-bucket-name
GCP_LOCATION=us-central1
GCP_CREDENTIALS_FILE=my-creds.json

# Airflow Web UI
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
```

### 2. Configurar Credenciales de GCP

1. Descarga tu archivo de credenciales de GCP (JSON) desde la consola de Google Cloud
2. Col√≥calo en la carpeta `gcp_keys/` con el nombre que especifiques en `GCP_CREDENTIALS_FILE` (por defecto: `my-creds.json`)
3. Aseg√∫rate de que el archivo tenga los permisos necesarios para:
   - Crear buckets en GCS
   - Crear datasets y tablas en BigQuery
   - Subir archivos a GCS

### 3. Inicializar Airflow

```bash
# Establecer el UID de Airflow (solo la primera vez)
export AIRFLOW_UID=$(id -u)

# Inicializar la base de datos
docker-compose up airflow-init

# Iniciar los servicios
docker-compose up -d
```

### 4. Acceder a la Interfaz Web

Abre tu navegador en: http://localhost:8080

- Usuario: `airflow` (o el que configuraste en `.env`)
- Contrase√±a: `airflow` (o la que configuraste en `.env`)

## üìù DAGs Disponibles

### 1. `07_gcp_setup`

**Descripci√≥n**: Configura los recursos iniciales de GCP (bucket y dataset).

**Uso**: 
- Ejecutar manualmente desde la UI de Airflow
- Se ejecuta una sola vez para configurar el entorno

**Tareas**:
- `create_gcs_bucket`: Crea el bucket en Google Cloud Storage
- `create_bq_dataset`: Crea el dataset en BigQuery

### 2. `08_gcp_taxi`

**Descripci√≥n**: Procesa datos de taxis (yellow o green) desde GitHub a BigQuery. Versi√≥n manual.

**Uso**: 
- Trigger manual desde la UI de Airflow
- Requiere configuraci√≥n JSON al ejecutar:

```json
{
  "taxi": "green",
  "year": "2021",
  "month": "01"
}
```

**Par√°metros**:
- `taxi`: `"yellow"` o `"green"`
- `year`: A√±o (ej: `"2019"`, `"2020"`, `"2021"`)
- `month`: Mes (ej: `"01"`, `"02"`, ..., `"12"`)

**Flujo**:
1. `extract`: Descarga y descomprime el archivo desde GitHub
2. `upload_to_gcs`: Sube el archivo a Google Cloud Storage
3. `branch_on_taxi_type`: Decide qu√© pipeline ejecutar (yellow o green)
4. Pipeline espec√≠fico (yellow o green):
   - Crear tabla principal
   - Crear tabla externa desde GCS
   - Crear tabla temporal con datos procesados
   - Merge de datos a la tabla principal
5. `cleanup_local_file`: Limpia el archivo local

### 3. `09_gcp_taxi_scheduled_green` y `09_gcp_taxi_scheduled_yellow`

**Descripci√≥n**: Versiones programadas que se ejecutan autom√°ticamente.

**Programaci√≥n**:
- **Green taxi**: D√≠a 1 de cada mes a las 9:00 AM (cron: `0 9 1 * *`)
- **Yellow taxi**: D√≠a 1 de cada mes a las 10:00 AM (cron: `0 10 1 * *`)

**Uso**: 
- Se ejecutan autom√°ticamente seg√∫n el schedule
- Usan la fecha de ejecuci√≥n (`execution_date`) para determinar el a√±o y mes a procesar

## üîÑ Comparaci√≥n con Kestra

| Caracter√≠stica | Kestra | Airflow |
|---------------|--------|---------|
| Configuraci√≥n | YAML | Python (DAGs) |
| Interfaz | Web UI | Web UI |
| Programaci√≥n | Cron en YAML | Cron en Python |
| Par√°metros | Inputs en YAML | `dag_run.conf` o `params` |
| Branching | `If` task | `BranchPythonOperator` |
| Variables | `{{vars.var}}` | `{{dag_run.conf.get()}}` o Jinja2 |
| Extensibilidad | Plugins | Operadores y Hooks |

## üìö Ejemplos de Uso

### Ejemplo 1: Procesar datos manualmente para enero 2021 (green taxi)

1. Ve a la UI de Airflow: http://localhost:8080
2. Encuentra el DAG `08_gcp_taxi`
3. Haz clic en "Trigger DAG w/ config"
4. Ingresa la configuraci√≥n JSON:
```json
{
  "taxi": "green",
  "year": "2021",
  "month": "01"
}
```
5. Haz clic en "Trigger"

### Ejemplo 2: Backfill para 2021

Para procesar todos los meses de 2021, puedes:

1. **Opci√≥n A**: Ejecutar manualmente el DAG `08_gcp_taxi` para cada combinaci√≥n de mes y tipo de taxi
2. **Opci√≥n B**: Usar el comando de Airflow CLI para backfill (si est√° habilitado):
```bash
docker-compose exec airflow-scheduler airflow dags backfill 08_gcp_taxi \
  --start-date 2021-01-01 \
  --end-date 2021-07-31 \
  --conf '{"taxi": "green", "year": "2021", "month": "01"}'
```

### Ejemplo 3: Verificar logs

```bash
# Ver logs del scheduler
docker-compose logs -f airflow-scheduler

# Ver logs del webserver
docker-compose logs -f airflow-webserver

# Ver logs de una tarea espec√≠fica
# (desde la UI de Airflow, haz clic en la tarea y luego en "Log")
```

## üõ†Ô∏è Comandos √ötiles

```bash
# Detener todos los servicios
docker-compose down

# Detener y eliminar vol√∫menes (¬°cuidado! elimina la base de datos)
docker-compose down -v

# Ver estado de los servicios
docker-compose ps

# Reiniciar un servicio espec√≠fico
docker-compose restart airflow-scheduler

# Ejecutar comandos de Airflow CLI
docker-compose exec airflow-scheduler airflow version
docker-compose exec airflow-scheduler airflow dags list
docker-compose exec airflow-scheduler airflow dags show 08_gcp_taxi
```

## üîç Troubleshooting

### Problema: Los DAGs no aparecen en la UI

**Soluci√≥n**:
1. Verifica que los archivos est√©n en la carpeta `dags/`
2. Revisa los logs del scheduler: `docker-compose logs airflow-scheduler`
3. Verifica que no haya errores de sintaxis en los DAGs
4. Reinicia el scheduler: `docker-compose restart airflow-scheduler`

### Problema: Error de autenticaci√≥n con GCP

**Soluci√≥n**:
1. Verifica que el archivo de credenciales (por defecto `my-creds.json`) est√© en `gcp_keys/`
2. Verifica que la variable `GOOGLE_APPLICATION_CREDENTIALS` est√© configurada correctamente en `docker-compose.yml` (debe apuntar a `/opt/airflow/dags/gcp_keys/{nombre-del-archivo}`)
3. Verifica que las credenciales tengan los permisos necesarios

### Problema: No se pueden descargar archivos

**Soluci√≥n**:
1. Verifica la conexi√≥n a internet desde el contenedor
2. Verifica que la URL de GitHub sea correcta
3. Revisa los logs de la tarea `extract`

## üìñ Recursos Adicionales

- [Documentaci√≥n de Apache Airflow](https://airflow.apache.org/docs/)
- [Airflow Providers para Google Cloud](https://airflow.apache.org/docs/apache-airflow-providers-google/)
- [Gu√≠a de Docker Compose para Airflow](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)

## üéØ Notas sobre el Homework

Para completar el homework del m√≥dulo 2 usando Airflow:

1. **Setup inicial**: Ejecuta `07_gcp_setup` una vez
2. **Procesar datos de 2021**: Usa `08_gcp_taxi` con las siguientes configuraciones:
   - Para cada mes de 2021 (01-07) y cada tipo de taxi (yellow, green)
   - Ejemplo para enero 2021 green: `{"taxi": "green", "year": "2021", "month": "01"}`
3. **Verificar resultados**: Consulta las tablas en BigQuery para responder las preguntas del quiz

## üìù Licencia

Este proyecto es parte del Data Engineering Zoomcamp 2026.
